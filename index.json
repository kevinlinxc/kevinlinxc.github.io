[{"content":"Blog posts and logbooks.\nA place to store information for myself and for the human colossus.\n","date":"15 November 2022","permalink":"/blogbook/","section":"Blogbook","summary":"Blog posts and logbooks.","title":"Blogbook"},{"content":"","date":"15 November 2022","permalink":"/","section":"Kevin Lin","summary":"","title":"Kevin Lin"},{"content":"","date":"25 October 2022","permalink":"/tags/design/","section":"Tags","summary":"","title":"design"},{"content":"","date":"25 October 2022","permalink":"/tags/engineering/","section":"Tags","summary":"","title":"engineering"},{"content":"My engineering experiences\n","date":"25 October 2022","permalink":"/engineering/","section":"Engineering","summary":"My engineering experiences","title":"Engineering"},{"content":"","date":"25 October 2022","permalink":"/tags/leadership/","section":"Tags","summary":"","title":"leadership"},{"content":"","date":"25 October 2022","permalink":"/tags/software/","section":"Tags","summary":"","title":"software"},{"content":"","date":"25 October 2022","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Summary # UBC AeroDesign (UBCAD) is the University of British Columbia\u0026rsquo;s airplane design team with over 50+ members. I\u0026rsquo;ve been on the team since 2020, and I\u0026rsquo;m currently the software lead. My time on this team has been very rewarding: it has been both a source of a lot of my growth as an engineer and the perfect place for me to apply and share my expertise. Read on to see the problems I solved.\nCompetition # The SAE AeroDesign competition during my time with the team was complex and intriguing.\nTeams had to build a large primary aircraft whose main goal was to deliver water to simulate a wildfire fight. The primary aircraft also carries a smaller aircraft called the PADA (Powered Autonomous Delivery Aircraft). You can see our 2022 planes in this picture I took:\nThe field that the aircrafts fly over has colored discs laid on the ground. Without prior knowledge of the location of these discs, teams have to deploy the PADA midair to land within a 15 ft radius of them. The competition also had an autonomous ground terrain vehicle portion too, but our team decided to focus on the PA and PADA as a scoring strategy.\nWhile I\u0026rsquo;ve been on the team, our team has only been able to physically go to competition once, in 2022, due to the COVID-19 pandemic. Unfortunately, UPS lost most of our Regular Class plane and some of our Advanced Class plane, so we only managed to eke out one flight round outside of the competition. Our sights are set on the 2023 competition, and I am confident that we\u0026rsquo;re going to be highly competitive with the progress our team has made, including in the software department.\n2022 Work # Our team is going to compete in the 2023 competition, as such, I won\u0026rsquo;t share too much until afterwards! For now, here\u0026rsquo;s some work I did that will probably not be critical to our strategy.\nOptical flow # During the preliminary design phase every year, the team tries out novel projects for a short period of time to encourage innovation and to suss out good ideas we might want to incorporate into the detailed design. One idea we had for the software was to determine the ground speed using optical flow algorithms, and so I spent a day figuring out how to use it, making this prototype:\n2021 Work # Python Dashboard # The majority of my time in 2021 was spent building a new live avionics dashboard application in Python.\nThis was part of a greater effort to move from the legacy architecture to a new one: Our reasoning for the transition was simply that Python is much easier to understand than React and JavaScript, which is ideal for onboarding new members and development in general.\nAfter making this architecture decision, I started working on the new dashboard and within a few months, I had a product I was pretty proud of: After another few months and some integration work, here is where I was at: As the 2022 competition approached, I spent lots of time tweaking the dashboard, incorporating functionality that other members and I had worked on. The most recent (but no longer used) version of the dashboard looks like this:\nThe dashboard here is in recording mode, where we can see the camera and sensor data, detect targets using CV, and tell the plane where and when to drop the plane.\nThis second image shows replay mode, where we can upload a CSV to see all of the flight data. This is both a competition requirement, and nice to have to be able to view data.\nAlthough I did write all of the code myself, a lot of high-quality feel can be attributed to Streamlit, the package I used for the frontend. Still, I solved many problems in many domains while making this site and integrating other things into this site, including:\nGraphing with the Altair library Networking/communicating with cellular data and 5Ghz Radio using ZeroMQ A brief investigation into publisher/subscriber models in Python Communicating over serial Logging data to a CSV files as efficiently as possible Caching parts of UI to save time Creating a CI/CD pipeline to ensure code was properly documented and formatted Ultimately, the app, being a webapp made in Python, did not perform as fast as we needed it to, so we ended up doing yet another redesign in 2022, which I also spearheaded.\nBlender simulation # For several reasons, it was difficult to get a nice video that we could use to train our CV/ML models. The video needed to be from the plane\u0026rsquo;s point of view, facing the ground, flying over targets.\nI was learning to use Blender, a well known modelling and animation software, and it struck me that it could be a way to generate the flight video until we got a real one.\nThis was the result of my work (the model of the plane I received had hollow wings): The goal was to make it easy to generate new, random videos with variable parameters, and that\u0026rsquo;s what I achieved:\nWe used the videos generated by Blender for almost a year until we got our hands on a drone and made suitable disc targets to fly over.\nComputer vision # I spent a a day prototyping a simple HSV color picker to demonstrate the viability of using a mask to isolate targets against a grassy field. There wasn\u0026rsquo;t much low-level optimization to do on the computer vision side at this point because we didn\u0026rsquo;t have data to test our algorithms on.\n2020 Work # I joined the team in 2020, and quickly got to work. Here\u0026rsquo;s a block diagram of the system at the time, for context:\nGround Station # My main contribution in 2020 was my work on our React avionics dashboard, labelled Groundstation Website.\nThe dashboard satisfied the competition requirement to display our plane\u0026rsquo;s altitude at all times, and also had information displayed to help a person on the ground decide when to release the plane\u0026rsquo;s payload.\nThe dashboard was made by a former team member, who used React as his frontend tool of choice. React had a fairly large learning curve, which meant that it was hard for me to make progress, especially since the former team member had left, but I managed to add some useful features.\nHere is a before-after comparison for my work:\nBefore: After:\nThe main features I added were:\nData visualization. Although not a competition requirement, this was very nice to have. The library I used was Recharts, and abig challenge was adding a brush and having the graph resize dynamically as more data points came in to keep the data easy to read. The ability to upload a CSV of data and play/stop the data to easily test the website in isolation. Previously, the site relied on connecting to the rest of the system, but since COVID-19 lockdown prevented us from getting our hardware, making this fake data generator was essential for progress to be made. Better icons for the map UI. Previously, the payload landing predictions and plane were all just circles, so I improved the clarity here. I achieved this using Bing ArcGIS Android App # The way we transmitted data from the plane to the ground was via an Android phone and cellular data. This is easy to make fun of because if my Air Canada flight was using an cell phone as a communicator, I would sue them, however, it got the job done and using cellular data was smart as it isn\u0026rsquo;t as finnicky as radio.\nFor the Android app, I made a few UI improvements, visible below:\nThe main feature is that I added a slider for the refresh rate of the phone, because setting that to 0 or a high number crashed the app. I also added constraints to all of the display widgets, as the layout values were hard-coded before then. We abandoned using a phone fairly quickly after I joined, so there wasn\u0026rsquo;t much opportunity to improve it.\nWebsocket Server # The Websocket Server was a Java layer that was the middle-man between the cellular data and the React app. Like everything else, it was legacy code made by the former member.\nI had to make several changes to this applet to enable the fake data generation I mentioned in the ground station section. To do so, I had to learn how to build a Java app into a .jar file, and I wrote documentation to help whoever would be in my position next. We also scrapped this part of the system, but I\u0026rsquo;m glad I learned what I did.\n","date":"25 October 2022","permalink":"/engineering/ubc-aerodesign/","section":"Engineering","summary":"SAE Student Engineering Design Team","title":"UBC AeroDesign"},{"content":"My projects that are software-based, which is like, all of them.\n","date":"27 September 2022","permalink":"/programming/","section":"Programming","summary":"My projects that are software-based, which is like, all of them.","title":"Programming"},{"content":"QuickFormula # Quickformula is a promising idea I have for a new project. My goal is to make the only site that we\u0026rsquo;ll ever need to use for formulas.\nI want to make it take seconds to find the formula you know is out there on the internet, rather than a minute of scrolling through Google results, images, and textbooks.\nI\u0026rsquo;ve flip flopped on how to achieve this. I started with a React implementation, switched to a Ruby on Rails after picking it up at Shopify, but both of those still had to steep a learning curve. For now, I think I\u0026rsquo;ll make the website out of Python and Flask, using one of the many templates and plugins.\nI\u0026rsquo;ll update this page once I make more progress - I\u0026rsquo;m very excited to work more on this.\n","date":"27 September 2022","permalink":"/programming/quickformula/","section":"Programming","summary":"My idea for the best STEM formula website. Currently in very development.","title":"QuickFormula"},{"content":"","date":"21 August 2022","permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"computer vision"},{"content":"Video demonstrating me detecting all the notes being played in a Synthesia render: Source code\nBackground # MIDI Files (Musical Instrument Digital Interface) files are a standard way to represent music. They are a sequence of notes, each with a pitch, duration, and volume.\nSynthesia is a program commonly used to render MIDI files to make these \u0026ldquo;tutorial\u0026rdquo; videos that you might have seen on YouTube: Sheet music is another way of storing music, which is how almost all musicians read music. It looks like this: The Problem # There are many YouTube videos that are the Synthesia or Synthesia-styled videos. These show off how the song is played, but learning from them is not easy. The creators usually use these to sell the sheet music that they\u0026rsquo;ve also created.\nWhile I would love to support these creators, the sheet music is usually 10 dollars for each sheet, which can add up to a lot of money over many songs. I realized that all of the information is theoretically already in the video, so I wanted to see if I could extract it using computer vision.\nDesign # The engineering behind this project is not too complicated. Basically, on most Synthesia videos, the keys light up when they are being played. So, I simply use OpenCV, convert frames to HSV, and detect green/blue pixels.\nFrom there, I use a Python MIDI library to generate my MIDI. One interesting thing to note is that I have to maintain some sort of state while I\u0026rsquo;m iterating through the frames. The MIDI file needs to know the duration of each note, so I have to keep track of when notes are first pressed and released. It\u0026rsquo;s kind of like a LeetCode problem, but with actual real-life value.\nAfter creating the MIDI file, I pass it on to a program called MuseScore, which converts the MIDI file to sheet music.\nResults # The generated sheet music from my pipeline is\u0026hellip;disappointing.\nMy MIDI translated to sheet music by MuseScore: Actual sheet music: This example is especially bad because I didn\u0026rsquo;t remove the dead air at the start of the video, but you can still see that it is bad otherwise.\nThe biggest problem is that, despite having the right notes and the approximate correct durations, the technology for turning MIDI into high quality, readable sheet music isn\u0026rsquo;t quite there yet. The generated sheet music is very messy, and the notes are not always aligned with the correct beats. When they are, they might not be placed in a way that humans would write them, which makes them hard to read. In the above example, the proper sheet music author uses the octave notation to make reading the sheet music easier, whereas the automatic conversion just puts the notes raw onto the staff.\nI tried using a different program, Sybelius, in place of MuseScore as it is supposedly better, but the results were still nothing close to human-made sheet music.\nAlthough this is disappointing, to me, it represents an opportunity to improve the technology.\nThe Knowledge Gap # All good scientists acknowledge areas where they or others could further investigate problems. For this project, that gap is the translation from MIDI to sheet music.\nWe have so much sheet music that we can turn into MIDI files, and this means lots of data that a machine learning model could use to figure out the inverse relationship. I\u0026rsquo;m not quite sure how to quantify the placement of notes on the sheet music side, so I\u0026rsquo;d really have to dive into the literature to see what\u0026rsquo;s out there.\nI plan on eventually trying to fill this gap, but for now I\u0026rsquo;m going to focus on other projects and pay the $10 each time I really want sheet music.\n","date":"21 August 2022","permalink":"/programming/desynthesia/","section":"Programming","summary":"Turning piano tutorials into sheet music","title":"DeSynthesia"},{"content":"","date":"4 September 2021","permalink":"/tags/electrical/","section":"Tags","summary":"","title":"electrical"},{"content":"","date":"4 September 2021","permalink":"/tags/integration/","section":"Tags","summary":"","title":"integration"},{"content":"Summary # UBC Rover (formerly known as UBC Snowbots) is the University of British Columbia\u0026rsquo;s autonomous rover design team. I was on the team from my first year at university (2018) until I left the team in 2021. This was my first experience working on an interdisciplinary engineering team, and I definitely owe a lot of my engineering experience to the team. I eventually rose to the role of integration lead, meaning I was interfacing all the components that the team was working on.\n2018-2019 Work # When I joined the team, I was initially placed on the electrical team, as I didn\u0026rsquo;t have much software experience yet.\nUnfortunately, I didn\u0026rsquo;t take many pictures or videos of my early work while I was on Snowbots. Some things I remember from these two years was:\nLearning about the existing power circuit, and learning how e-stops and relays worked for the first time Researching suitable light, non-flammable materials to use as the base of our power circuit Recreated the power circuit using simpler parts (Arduinos, thinner wires) and low power Learning to strip and crimp wires Troubleshooted wireless \u0026ldquo;Compex\u0026rdquo; brand router that we sourced that wasn\u0026rsquo;t working for a long time 2020-2021 # Software # As 2020 reared its ugly head, I switched to the software team as the rover software interested me greatly.\nThis role was where I learned complex Git workflows like forking, using Continuous Integration (we used TravisCI), and also where I learned to use Linux and bash.\nROS # The main software used at Snowbots was a framework called ROS, or Robot Operating System. It\u0026rsquo;s very commonly used in many robotics operations because it allows easy to understand communication between many parts of a system. ROS comes in two flavours, Python and C++, and our team used C++ due to the increased speed and reliability.\nPro Controller # One of my main tasks was to receive commands in ROS (C++) from a Nintendo Switch Pro Controller. There was already existing support in ROS for Xbox and Playstation controllers, but I figured it wouldn\u0026rsquo;t be hard to add this new controller. Spoilers: it was pretty hard.\nIf I remember correctly, I had to install several drivers that eventually created an xinput device that I could detect with the Linux command line, and eventually that translated to being able to see the commands in C++ and ROS.\nAs a demonstration of my work, I managed to use a model made by a previous member in the simulation software Gazebo, and controlled its movement using the controller (apologies for the low resolution):\nPhidget Motors # Similar to the Pro Controller, another task I had was to interface ROS C++ Code with Phidget brand brushless DC motors. This task was easier than the controller because the Phidget C++ library was documented on their website, however, figuring out all of the electrical connections proved to be tricky. Ultimately, this was just a matter of reading documentation and reaching out to their support whenever I got blocked.\nA picture of all the motors, their drivers, and their batteries:\nIntegration # As I was growing in seniority within the team and gaining multidisciplinary experience from my program, it made sense to move into an integration role. In fact, that was pretty much already what I was doing by controlling the motors with ROS.\nMotor - Controller integration # One of the first things I did that furthered the integration of our systems was drive the BLDC motors with the Pro Controller:\nIn essence, the Pro Controller signals are being translated to an xinput device which is getting translated to a ROS Twist message by the code I wrote in 2020, which is then being converted into individual commands for the 6 motors using the code I wrote in 2021.\nI did some further integration with our wireless routers and our Intel NUC, the computer that would actually be on our rover, resulting in this awesome integration test where our Pro Controller was controlling the motors without being connected to the same computer:\nFull drive control + networking integration # Leaving the team # Shortly after doing this last end-to-end test, I left the team to put more time into UBC AeroDesign. The mechanical team of Snowbots had stagnated and hadn\u0026rsquo;t built anything while I was on the team, partially due to COVID, so I was hoping to see a faster design cycle with Aero. The team went on to do well without me, and you can see my motor and controller work in action in this promotional video:\n","date":"4 September 2021","permalink":"/engineering/ubc-rover/","section":"Engineering","summary":"Student Engineering Design team that works on autonomous land rovers","title":"UBC Rover"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"\nThis page is my version of a video editing portfolio, an annotated showcase of some of my favourite work.\n2022 # An animation of my university program\u0026rsquo;s logo, from scratch. Motion tracked custom graphics for tweets and messages. Notice the post button\u0026rsquo;s response when \u0026ldquo;tapped\u0026rdquo; :) A dolly zoom using a DSLR, a projector cart, and a table as a guide 2021 # An Invincible title card tutorial Minecraft-style credits I made for FilmfEUSt 2021 One of the nicer title cards I\u0026rsquo;ve ever made 2020 # A Naruto-style opening I made for my favourite hockey team, the Vancouver Canucks. A League of Legends parody of the \u0026ldquo;Avatar: The Last Airbender\u0026rdquo; intro 2019 # A League of Legends parody of the iCarly intro. I had to manually corner pin the clips for this, easily 1000 keyframes\nA Marvel-style title card for League of Legends, made following a tutorial with Cinema4D and the Adobe Suite\nA Marvel Infinity War trailer title card for League of Legends, also made following a tutorial with Cinema4D Inspiration # The first video I ever made was actually in Grade 5, with help from my dad. Our class had a \u0026ldquo;challenge project\u0026rdquo; where we could go above and beyond a normal assignment for extra credit. My dad, my friend and I visited a ghost town in our province and recorded a whole tour with a cute storyline. Back at home, my dad helped us do add some simple graphics and a voiceover in iMovie. I definitely owe my interest in filmmaking to him.\nAnother reason I got into video editing was this animation from \u0026ldquo;To This Day\u0026rdquo;, a stand up poetry video about bullying. I was shown this in Grade 8, and I was really impressed by how the creator had managed to create such a smooth, aesthetically-pleasing animation.\nVideo editing to me has always been about creating impressive and valuable visuals out of thin air, and this was my first exposure to something that made me want to create art.\n","date":"1 January 0001","permalink":"/video_editing/","section":"Kevin Lin","summary":"","title":"Video Editing"}]